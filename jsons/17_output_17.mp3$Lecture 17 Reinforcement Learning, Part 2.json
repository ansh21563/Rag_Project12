{
  "chunks": [
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 0.0,
      "end": 18.28,
      "text": " So we're going to have a three-part lecture today, still continuing on the theme of reinforcement"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 18.28,
      "end": 20.32,
      "text": " learning."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 20.32,
      "end": 26.36,
      "text": " Part one, I'm going to be speaking, and I'll be following up on last week's discussion"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 26.36,
      "end": 29.84,
      "text": " about causal inference and Tuesday's discussion on reinforcement learning."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 29.84,
      "end": 37.76,
      "text": " And I'll be going into one more subtlety that arises there and where we can develop some"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 37.76,
      "end": 40.74,
      "text": " nice mathematical methods to help with."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 40.74,
      "end": 46.28,
      "text": " And then I'm going to turn over the show to Barbara, who I'll formally introduce when"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 46.28,
      "end": 47.72,
      "text": " the time comes."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 47.72,
      "end": 55.08,
      "text": " And she's going to both talk about some of her work on developing and evaluating dynamic"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 55.08,
      "end": 56.879999999999995,
      "text": " treatment regimes."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 56.879999999999995,
      "end": 61.44,
      "text": " And then she'll lead a discussion on the sepsis paper, which was the required reading from"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 61.44,
      "end": 62.64,
      "text": " today's class."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 62.64,
      "end": 67.92,
      "text": " So those are the three parts of today's lecture."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 67.92,
      "end": 72.56,
      "text": " So I want you to return back, put yourself back in the mindset of Tuesday's lecture,"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 72.56,
      "end": 74.12,
      "text": " where we talked about reinforcement learning."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 74.48,
      "end": 85.0,
      "text": " And remember that the goal of reinforcement learning was to optimize some reward."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 85.0,
      "end": 96.52000000000001,
      "text": " Specifically, our goal was to find some policy, which I can denote as pi star, which is the"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 96.52,
      "end": 106.72,
      "text": " arg max over all possible policies pi of v of pi, where, just to remind you, v of pi"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 106.72,
      "end": 110.0,
      "text": " is the value of the policy pi."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 110.0,
      "end": 122.19999999999999,
      "text": " Formally it's defined as the expectation of the sum of the rewards across time."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 122.52,
      "end": 127.28,
      "text": " The reason why I'm calling this an expectation, it's like the pi, is because there's stochasticity"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 127.28,
      "end": 132.96,
      "text": " both in the environment and possibly pi is going to be a stochastic policy."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 132.96,
      "end": 138.6,
      "text": " And this is summing over the time steps, because this is not just a single time step problem,"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 138.6,
      "end": 142.96,
      "text": " but we're going to be considering interventions across time of the reward at each point in"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 142.96,
      "end": 143.96,
      "text": " time."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 143.96,
      "end": 146.64000000000001,
      "text": " And the reward function could either be at each point in time, or you might imagine that"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 146.64000000000001,
      "end": 152.04,
      "text": " this is 0 for all time steps except for the last time step."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 152.88,
      "end": 157.35999999999999,
      "text": " So the first question I want us to think about is, well, what are the implications of this"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 157.35999999999999,
      "end": 160.6,
      "text": " as a learning paradigm?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 160.6,
      "end": 166.48,
      "text": " If we look what's going on over here, hidden in my story is also an expectation over x,"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 166.48,
      "end": 170.12,
      "text": " the patient, for example, or the initial state."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 170.12,
      "end": 177.44,
      "text": " And so this intuitively is saying, let's try to find a policy that has high expected reward,"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 177.44,
      "end": 180.04,
      "text": " average nessance over all patients."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 181.04,
      "end": 186.56,
      "text": " And I just wanted you to think about whether that is indeed the right goal."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 186.56,
      "end": 189.48,
      "text": " Can anyone think about a setting where that might not be desirable?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 189.48,
      "end": 190.48,
      "text": " Yeah?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 190.48,
      "end": 198.48,
      "text": " What if the reward is the patient living or dying?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 198.48,
      "end": 202.48,
      "text": " You don't want it to have high variance, like saving a few patients and then 0, but then"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 202.48,
      "end": 204.79999999999998,
      "text": " in expected sense having some."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 205.08,
      "end": 212.4,
      "text": " What happens if this reward is something mission critical, like patient dying?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 212.4,
      "end": 216.54000000000002,
      "text": " You really want to try to avoid that from happening as much as possible."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 216.54000000000002,
      "end": 219.8,
      "text": " Of course, there are other criteria that we might be interested in as well."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 219.8,
      "end": 224.24,
      "text": " And both in Frederick's lecture on Tuesday and in the readings, we talked about how there"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 224.24,
      "end": 231.68,
      "text": " might be other aspects about making sure that a patient is not just alive, but also healthy,"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 231.68,
      "end": 233.20000000000002,
      "text": " which might play into reward function."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 233.2,
      "end": 236.0,
      "text": " So there might be rewards associated with those."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 236.0,
      "end": 240.76,
      "text": " And if you were to just, for example, put a positive or negative infinity for a patient"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 240.76,
      "end": 243.48,
      "text": " dying, that's a non-starter."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 243.48,
      "end": 248.35999999999999,
      "text": " Because if you did that, unfortunately, in this world, we're not always going to be able"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 248.35999999999999,
      "end": 249.95999999999998,
      "text": " to keep patients alive."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 249.95999999999998,
      "end": 253.48,
      "text": " And so you're going to get into an infeasible optimization problem."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 253.48,
      "end": 255.28,
      "text": " So minus infinity is not an option."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 255.28,
      "end": 260.48,
      "text": " We're going to have to put some number to it in this type of approach."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 260.48,
      "end": 264.76,
      "text": " And then you're going to start trading off between patients."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 264.76,
      "end": 271.64000000000004,
      "text": " In some cases, you might have a very high reward for it."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 271.64000000000004,
      "end": 275.04,
      "text": " There are two different solutions that you can imagine."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 275.04,
      "end": 280.20000000000005,
      "text": " One solution where the reward is somewhat balanced across patients, and another situation"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 280.20000000000005,
      "end": 284.44,
      "text": " where you have really small values of reward for some patients and a few patients with"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 284.44,
      "end": 285.8,
      "text": " very large values of rewards."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 285.8,
      "end": 288.40000000000003,
      "text": " And both would give you the same average, obviously."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 289.15999999999997,
      "end": 291.96,
      "text": " But both are not necessarily equally useful."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 291.96,
      "end": 296.64,
      "text": " We might want to say that we prefer to avoid that worst case situation."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 296.64,
      "end": 300.56,
      "text": " So one could imagine other ways of formulating this optimization problem."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 300.56,
      "end": 305.03999999999996,
      "text": " Maybe you want to control the worst case reward instead of the average case reward."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 305.03999999999996,
      "end": 308.12,
      "text": " Or maybe you want to say something about different quartiles."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 308.12,
      "end": 312.91999999999996,
      "text": " And I just wanted to point that out, because really, that's the starting place for a lot"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 312.91999999999996,
      "end": 316.32,
      "text": " of the work that we're doing here."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 316.32,
      "end": 324.84,
      "text": " So now I want us to think through, OK, returning back to this goal, we've done our policy iteration."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 324.84,
      "end": 327.12,
      "text": " Or we've done our Q-learning, that is."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 327.12,
      "end": 329.2,
      "text": " And we get a policy out."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 329.2,
      "end": 332.4,
      "text": " And we might now want to know, well, what is the value of that policy?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 332.4,
      "end": 335.92,
      "text": " So what is our estimate of that quantity?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 335.92,
      "end": 341.68,
      "text": " Well, to get that, one could just try to read it off from the results of Q-learning by just"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 341.68,
      "end": 348.24,
      "text": " computing that v pi, what I'm going to call v pi hat, the estimate, is just equal to now"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 348.24,
      "end": 359.76,
      "text": " a maximum over actions a of your Q function evaluated at whatever your initial state is"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 359.76,
      "end": 363.68,
      "text": " and the optimal choice of action a."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 363.68,
      "end": 368.64,
      "text": " So all I'm saying here is that the last step of the algorithm might be to ask, well, what"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 368.64,
      "end": 372.08,
      "text": " is the expected reward of this policy?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 372.08,
      "end": 375.47999999999996,
      "text": " And if you remember, the Q-learning algorithm is, in essence, a dynamic programming algorithm"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 375.47999999999996,
      "end": 381.32,
      "text": " working its way from the large values of time up to the present."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 381.32,
      "end": 385.8,
      "text": " And it is, indeed, actually computing this expected value that you're interested in."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 385.8,
      "end": 390.76,
      "text": " So you could just read it off from the Q values at the very end."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 390.76,
      "end": 394.47999999999996,
      "text": " But I want to point out that here, there is an implicit policy built in."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 394.56,
      "end": 399.72,
      "text": " So I'm going to compare this in just a second to what happens under the causal inference"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 399.72,
      "end": 405.40000000000003,
      "text": " scenario, so just a single time step in potential outcomes framework that we're used to."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 405.40000000000003,
      "end": 412.96000000000004,
      "text": " Notice that the value of this policy, the reason why it's a function of pi is because"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 412.96000000000004,
      "end": 418.56,
      "text": " the value is a function of every subsequent action that you're taking as well."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 418.8,
      "end": 424.64,
      "text": " So now let's just compare that for a second to what happens in the potential outcomes"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 424.64,
      "end": 428.16,
      "text": " framework."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 428.16,
      "end": 430.64,
      "text": " So there, our starting place."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 430.64,
      "end": 440.4,
      "text": " So now I'm going to turn our attention for just one moment from reinforcement learning"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 440.4,
      "end": 444.28,
      "text": " now back to just causal inference."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 444.28,
      "end": 446.7,
      "text": " In reinforcement learning, we talked about policies."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 446.84,
      "end": 453.14,
      "text": " How do we find policies to do well in terms of some expected reward of this policy?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 453.14,
      "end": 460.53999999999996,
      "text": " But yet when we were talking about causal inference, we only used words like average"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 460.53999999999996,
      "end": 467.18,
      "text": " treatment effect or conditional average treatment effect, where, for example, to estimate the"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 467.18,
      "end": 471.06,
      "text": " conditional average treatment effect, what we said is we're going to first learn, if"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 471.14,
      "end": 478.66,
      "text": " we use a covariate adjustment approach, we learn some function f of x comma t, which"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 478.66,
      "end": 488.42,
      "text": " is intended to be an approximation of the expected value of your outcome y given x comma"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 488.42,
      "end": 498.54,
      "text": " I'll say y of t."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 498.54,
      "end": 501.26,
      "text": " There, so that notation."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 501.26,
      "end": 505.38,
      "text": " So the goal of covariate adjustment was to estimate this quantity."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 505.38,
      "end": 509.54,
      "text": " And we could use that then to try to construct a policy."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 509.54,
      "end": 522.4200000000001,
      "text": " For example, you could think about the policy pi of x, which simply looks to see is, we'll"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 522.4200000000001,
      "end": 536.4200000000001,
      "text": " say it's 1 if your estimate of kate for x is positive and 0 otherwise."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 536.42,
      "end": 543.9799999999999,
      "text": " Just to remind you, the way that we got the estimate of kate for an individual x was just"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 543.98,
      "end": 570.62,
      "text": " by looking at f of x comma 1 minus f of x comma 0."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 570.62,
      "end": 575.78,
      "text": " So if we have a policy, so now we're going to start thinking about policies in the context"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 575.78,
      "end": 579.3,
      "text": " of causal inference, just like we were doing in reinforcement learning."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 579.3,
      "end": 586.82,
      "text": " And I want us to think through, what would the analogous value of the policy be?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 586.82,
      "end": 589.98,
      "text": " How good is that policy?"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 589.98,
      "end": 592.74,
      "text": " It could be another policy, but right now I'm assuming I'm just going to focus on this"
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 592.74,
      "end": 596.7,
      "text": " policy that I show up here."
    },
    {
      "number": "output",
      "title": "Lecture 17 Reinforcement Learning, Part 2",
      "start": 596.7,
      "end": 601.6600000000001,
      "text": " So one approach to try to evaluate how good that policy would be."
    }
  ],
  "text": " So we're going to have a three-part lecture today, still continuing on the theme of reinforcement learning. Part one, I'm going to be speaking, and I'll be following up on last week's discussion about causal inference and Tuesday's discussion on reinforcement learning. And I'll be going into one more subtlety that arises there and where we can develop some nice mathematical methods to help with. And then I'm going to turn over the show to Barbara, who I'll formally introduce when the time comes. And she's going to both talk about some of her work on developing and evaluating dynamic treatment regimes. And then she'll lead a discussion on the sepsis paper, which was the required reading from today's class. So those are the three parts of today's lecture. So I want you to return back, put yourself back in the mindset of Tuesday's lecture, where we talked about reinforcement learning. And remember that the goal of reinforcement learning was to optimize some reward. Specifically, our goal was to find some policy, which I can denote as pi star, which is the arg max over all possible policies pi of v of pi, where, just to remind you, v of pi is the value of the policy pi. Formally it's defined as the expectation of the sum of the rewards across time. The reason why I'm calling this an expectation, it's like the pi, is because there's stochasticity both in the environment and possibly pi is going to be a stochastic policy. And this is summing over the time steps, because this is not just a single time step problem, but we're going to be considering interventions across time of the reward at each point in time. And the reward function could either be at each point in time, or you might imagine that this is 0 for all time steps except for the last time step. So the first question I want us to think about is, well, what are the implications of this as a learning paradigm? If we look what's going on over here, hidden in my story is also an expectation over x, the patient, for example, or the initial state. And so this intuitively is saying, let's try to find a policy that has high expected reward, average nessance over all patients. And I just wanted you to think about whether that is indeed the right goal. Can anyone think about a setting where that might not be desirable? Yeah? What if the reward is the patient living or dying? You don't want it to have high variance, like saving a few patients and then 0, but then in expected sense having some. What happens if this reward is something mission critical, like patient dying? You really want to try to avoid that from happening as much as possible. Of course, there are other criteria that we might be interested in as well. And both in Frederick's lecture on Tuesday and in the readings, we talked about how there might be other aspects about making sure that a patient is not just alive, but also healthy, which might play into reward function. So there might be rewards associated with those. And if you were to just, for example, put a positive or negative infinity for a patient dying, that's a non-starter. Because if you did that, unfortunately, in this world, we're not always going to be able to keep patients alive. And so you're going to get into an infeasible optimization problem. So minus infinity is not an option. We're going to have to put some number to it in this type of approach. And then you're going to start trading off between patients. In some cases, you might have a very high reward for it. There are two different solutions that you can imagine. One solution where the reward is somewhat balanced across patients, and another situation where you have really small values of reward for some patients and a few patients with very large values of rewards. And both would give you the same average, obviously. But both are not necessarily equally useful. We might want to say that we prefer to avoid that worst case situation. So one could imagine other ways of formulating this optimization problem. Maybe you want to control the worst case reward instead of the average case reward. Or maybe you want to say something about different quartiles. And I just wanted to point that out, because really, that's the starting place for a lot of the work that we're doing here. So now I want us to think through, OK, returning back to this goal, we've done our policy iteration. Or we've done our Q-learning, that is. And we get a policy out. And we might now want to know, well, what is the value of that policy? So what is our estimate of that quantity? Well, to get that, one could just try to read it off from the results of Q-learning by just computing that v pi, what I'm going to call v pi hat, the estimate, is just equal to now a maximum over actions a of your Q function evaluated at whatever your initial state is and the optimal choice of action a. So all I'm saying here is that the last step of the algorithm might be to ask, well, what is the expected reward of this policy? And if you remember, the Q-learning algorithm is, in essence, a dynamic programming algorithm working its way from the large values of time up to the present. And it is, indeed, actually computing this expected value that you're interested in. So you could just read it off from the Q values at the very end. But I want to point out that here, there is an implicit policy built in. So I'm going to compare this in just a second to what happens under the causal inference scenario, so just a single time step in potential outcomes framework that we're used to. Notice that the value of this policy, the reason why it's a function of pi is because the value is a function of every subsequent action that you're taking as well. So now let's just compare that for a second to what happens in the potential outcomes framework. So there, our starting place. So now I'm going to turn our attention for just one moment from reinforcement learning now back to just causal inference. In reinforcement learning, we talked about policies. How do we find policies to do well in terms of some expected reward of this policy? But yet when we were talking about causal inference, we only used words like average treatment effect or conditional average treatment effect, where, for example, to estimate the conditional average treatment effect, what we said is we're going to first learn, if we use a covariate adjustment approach, we learn some function f of x comma t, which is intended to be an approximation of the expected value of your outcome y given x comma I'll say y of t. There, so that notation. So the goal of covariate adjustment was to estimate this quantity. And we could use that then to try to construct a policy. For example, you could think about the policy pi of x, which simply looks to see is, we'll say it's 1 if your estimate of kate for x is positive and 0 otherwise. Just to remind you, the way that we got the estimate of kate for an individual x was just by looking at f of x comma 1 minus f of x comma 0. So if we have a policy, so now we're going to start thinking about policies in the context of causal inference, just like we were doing in reinforcement learning. And I want us to think through, what would the analogous value of the policy be? How good is that policy? It could be another policy, but right now I'm assuming I'm just going to focus on this policy that I show up here. So one approach to try to evaluate how good that policy would be."
}
