{
    "chunks": [
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 0.0,
            "end": 33.24,
            "text": " So today's lecture is going to be about causality.  Who's heard about causality before?  Raise your hand.  What's the number one thing that you  hear about when thinking about causality?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 33.24,
            "end": 41.480000000000004,
            "text": " Yeah.  Correlation does not imply causation.  Correlation does not imply causation.  Anything else come to mind?  That's what came to my mind."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 41.480000000000004,
            "end": 56.16,
            "text": " Anything else come to mind?  So up until now in the semester, we've  been talking about purely predictive questions.  And for purely predictive questions,  one could argue that correlation is good enough."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 56.16,
            "end": 70.0,
            "text": " If we have some signs in our data  that are predictive of some outcome of interest,  we want to be able to take advantage of that, whether it's  upstream, downstream.  The causal directionality is irrelevant for that purpose."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 70.0,
            "end": 83.4,
            "text": " Although even that isn't quite true,  because I've been hinting throughout the semester,  people have been hinting throughout the semester,  that there are times when the data changes on you.  For example, when you go from one institution to another,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 83.4,
            "end": 97.2,
            "text": " or when you have non-stationarity.  And in those situations, having a deeper understanding  about the data might allow one to build  an additional robustness to that type of data set shift.  But there are other reasons as well"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 97.2,
            "end": 107.6,
            "text": " why understanding something about your underlying data  generating process could be really important.  It's because often the questions that we  want to answer when it comes to health care  are not predictive questions."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 107.6,
            "end": 119.68,
            "text": " They're causal questions.  And so what I'll do now is I'll walk through a few examples  of what I mean by this.  I'm going to start out with what we saw in lecture four  and problem set two, where we looked"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 119.68,
            "end": 136.32,
            "text": " at the question of how we can do early detection of type 2  diabetes.  You used Truven MarketScan's data  set to build a risk stratification algorithm  for detecting who is going to be newly diagnosed with diabetes"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 136.32,
            "end": 150.08,
            "text": " one to three years from now.  And if you think about how one might then try to deploy  this algorithm, you might, for example,  try to get patients into the clinic to get them diagnosed.  But the next set of questions are usually"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 150.08,
            "end": 161.56,
            "text": " about the so what question.  What are you going to do based on that prediction?  Once diagnosed, how will you intervene?  And at the end of the day, the interesting goal  is not one of how do you find them early,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 161.56,
            "end": 176.52,
            "text": " but how do you prevent them from developing diabetes,  or how do you prevent the patient from developing  complications of diabetes?  And those are questions about causality.  Now, when we built our predictive models"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 176.52,
            "end": 190.06,
            "text": " and we introspected at the weight,  we might have noticed some interesting things.  For example, if you looked at the highest negative weight,  which I'm not sure if we did as part of the assignment,  but something that I did as part of my research study,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 190.06,
            "end": 206.0,
            "text": " you see that gastric bypass surgery  has the biggest negative weight.  Does that mean that if you give an obese person gastric bypass  surgery, that will prevent them from developing  type 2 diabetes?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 206.0,
            "end": 219.84,
            "text": " That's an example of a causal question which  is raised by this predictive model.  But just by looking at the weight alone,  as I'll show you this week, you won't  be able to correctly infer that there is a causal relationship."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 219.84,
            "end": 231.6,
            "text": " And so part of what we'll be doing  is coming up with a mathematical language  for thinking about how does one answer,  is there a causal relationship here?  Here's a second example."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 231.6,
            "end": 245.4,
            "text": " Right before spring break, we had a series of lectures  about diagnosis, particularly diagnosis  from imaging data of a variety of kinds,  whether it be radiology or pathology.  And often, questions are of this sort."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 245.4,
            "end": 259.52,
            "text": " Here is a woman's breast.  She has breast cancer.  Maybe you have an associated pathology slide as well.  And you want to know, what is the risk of this person dying  in the next five years?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 259.52,
            "end": 272.32,
            "text": " So one can take a deep learning model,  learn to predict what one observes.  So in the patients in your data set, you have the input.  You have, let's say, survival time.  And you might use that to predict something"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 272.32,
            "end": 294.04,
            "text": " about how long it takes from diagnosis to death.  And based on those predictions, you might take actions.  For example, if you predict that a patient is not risky,  then you might conclude that they  don't need to get treatment."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 294.04,
            "end": 311.96,
            "text": " But that could be really, really dangerous.  And I'll just give you one example  of why that could be dangerous.  These predictive models, if you're  learning them in this way, the outcome, in this case,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 311.96,
            "end": 324.4,
            "text": " let's say, time to death, is going  to be affected by what's happened in between.  So for example, this patient might  have been receiving treatment.  And because of them receiving treatment"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 324.4,
            "end": 337.32,
            "text": " in between the time from diagnosis to death,  it might have prolonged their life.  And so for this patient in your data set,  you might have observed that they lived a very long time.  But if you ignore what happens in between,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 337.46,
            "end": 348.56,
            "text": " and you simply learn to predict y from x, x being the input,  then a new patient comes along.  You predict that that new patient  is going to survive a long time.  And it would be completely the wrong conclusion"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 348.56,
            "end": 360.44,
            "text": " to say that you don't need to treat that patient.  Because in fact, the only reason the patients like them  in the training data lived a long time  is because they were treated.  And so when it comes to this field of machine learning"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 360.44,
            "end": 376.0,
            "text": " and health care, we need to think really carefully  about these types of questions.  Because an error in the way that we formalize our problem  could kill people because of mistakes like this.  Now, other questions are one about not"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 376.0,
            "end": 390.68,
            "text": " how do we predict outcomes, but how do we  guide treatment decisions.  So for example, as data from pathology  gets richer and richer and richer,  we might think that we can now use computers"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 390.68,
            "end": 408.28,
            "text": " to try to better predict who is likely to benefit  from a treatment than humans could do alone.  But the challenge with using algorithms to do that  is that people respond differently to treatment.  And the data which is being used to guide treatment"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 408.28,
            "end": 423.82,
            "text": " is biased based on existing treatment guidelines.  So similarly to the previous question,  we could ask what would happen if we trained to predict  past treatment decisions.  This would be the most naive way"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 423.82,
            "end": 437.54,
            "text": " to try to use data to guide treatment decisions.  So maybe you see David gets treatment A,  John gets treatment B, Juana gets treatment A.  And you might ask, then, OK, a new patient comes in.  What should this new patient be treated with?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 437.54,
            "end": 453.2,
            "text": " And if you just learned a model to predict from what you know  about David the treatment that David is likely to get,  then the best that you could hope to do  is to do as well as existing clinical practice.  So if we want to go beyond current clinical practice,"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 453.2,
            "end": 470.6,
            "text": " for example, to recognize that there is heterogeneity  in treatment response, then we have  to somehow change the question that we're asking.  I'll give you one last example, which is perhaps  a more traditional question of does X cause Y?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 470.6,
            "end": 487.4,
            "text": " For example, does smoking cause lung cancer  is a major question of societal importance.  Now, you might be familiar with the traditional way  of trying to answer questions of this nature, which  would be to do a randomized control trial."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 487.4,
            "end": 501.0,
            "text": " Except this isn't exactly the type of setting  where you could do a randomized control trial.  How would you feel if you were a smoker,  and someone came up to you and said,  you have to stop smoking because I need to see what happens?"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 501.0,
            "end": 516.36,
            "text": " Or how would you feel if you were a non-smoker,  and someone came up to you and said, you have to start smoking?  That would be both not feasible and completely unethical.  And so if we want to try to answer questions  like this from data, we need to start thinking about how"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 516.36,
            "end": 532.36,
            "text": " can we design, using observational data, ways  of answering questions like this.  And the challenge is that there's  going to be bias in the data because of who decides to smoke  and who decides not to smoke."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 532.36,
            "end": 541.8,
            "text": " So for example, the most naive way  you might try to answer this question  would be to look at the conditional likelihood  getting lung cancer among smokers  and getting lung cancer among non-smokers."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 544.56,
            "end": 566.68,
            "text": " But those numbers, as you'll see in the next few slides,  can be very misleading because there  might be confounding factors, factors that would, for example,  both cause people to be a smoker and cause  them to receive lung cancer, which"
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 566.7199999999999,
            "end": 577.4799999999999,
            "text": " would differentiate between these two numbers.  And we'll have a very concrete example  of this in just a few minutes.  So to properly answer all of these questions,  one needs to be thinking in terms of causal graphs."
        },
        {
            "number": "lec14",
            "title": "part.000.mp3",
            "start": 577.4799999999999,
            "end": 600.08,
            "text": " So rather than the traditional setup in machine learning,  where you just have inputs and outputs,  now we need to have triplets.  Rather than having inputs and outputs,  we need to be thinking of inputs, intervention."
        }
    ],
    "text": " So today's lecture is going to be about causality. Who's heard about causality before? Raise your hand. What's the number one thing that you hear about when thinking about causality? Yeah. Correlation does not imply causation. Correlation does not imply causation. Anything else come to mind? That's what came to my mind. Anything else come to mind? So up until now in the semester, we've been talking about purely predictive questions. And for purely predictive questions, one could argue that correlation is good enough. If we have some signs in our data that are predictive of some outcome of interest, we want to be able to take advantage of that, whether it's upstream, downstream. The causal directionality is irrelevant for that purpose. Although even that isn't quite true, because I've been hinting throughout the semester, people have been hinting throughout the semester, that there are times when the data changes on you. For example, when you go from one institution to another, or when you have non-stationarity. And in those situations, having a deeper understanding about the data might allow one to build an additional robustness to that type of data set shift. But there are other reasons as well why understanding something about your underlying data generating process could be really important. It's because often the questions that we want to answer when it comes to health care are not predictive questions. They're causal questions. And so what I'll do now is I'll walk through a few examples of what I mean by this. I'm going to start out with what we saw in lecture four and problem set two, where we looked at the question of how we can do early detection of type 2 diabetes. You used Truven MarketScan's data set to build a risk stratification algorithm for detecting who is going to be newly diagnosed with diabetes one to three years from now. And if you think about how one might then try to deploy this algorithm, you might, for example, try to get patients into the clinic to get them diagnosed. But the next set of questions are usually about the so what question. What are you going to do based on that prediction? Once diagnosed, how will you intervene? And at the end of the day, the interesting goal is not one of how do you find them early, but how do you prevent them from developing diabetes, or how do you prevent the patient from developing complications of diabetes? And those are questions about causality. Now, when we built our predictive models and we introspected at the weight, we might have noticed some interesting things. For example, if you looked at the highest negative weight, which I'm not sure if we did as part of the assignment, but something that I did as part of my research study, you see that gastric bypass surgery has the biggest negative weight. Does that mean that if you give an obese person gastric bypass surgery, that will prevent them from developing type 2 diabetes? That's an example of a causal question which is raised by this predictive model. But just by looking at the weight alone, as I'll show you this week, you won't be able to correctly infer that there is a causal relationship. And so part of what we'll be doing is coming up with a mathematical language for thinking about how does one answer, is there a causal relationship here? Here's a second example. Right before spring break, we had a series of lectures about diagnosis, particularly diagnosis from imaging data of a variety of kinds, whether it be radiology or pathology. And often, questions are of this sort. Here is a woman's breast. She has breast cancer. Maybe you have an associated pathology slide as well. And you want to know, what is the risk of this person dying in the next five years? So one can take a deep learning model, learn to predict what one observes. So in the patients in your data set, you have the input. You have, let's say, survival time. And you might use that to predict something about how long it takes from diagnosis to death. And based on those predictions, you might take actions. For example, if you predict that a patient is not risky, then you might conclude that they don't need to get treatment. But that could be really, really dangerous. And I'll just give you one example of why that could be dangerous. These predictive models, if you're learning them in this way, the outcome, in this case, let's say, time to death, is going to be affected by what's happened in between. So for example, this patient might have been receiving treatment. And because of them receiving treatment in between the time from diagnosis to death, it might have prolonged their life. And so for this patient in your data set, you might have observed that they lived a very long time. But if you ignore what happens in between, and you simply learn to predict y from x, x being the input, then a new patient comes along. You predict that that new patient is going to survive a long time. And it would be completely the wrong conclusion to say that you don't need to treat that patient. Because in fact, the only reason the patients like them in the training data lived a long time is because they were treated. And so when it comes to this field of machine learning and health care, we need to think really carefully about these types of questions. Because an error in the way that we formalize our problem could kill people because of mistakes like this. Now, other questions are one about not how do we predict outcomes, but how do we guide treatment decisions. So for example, as data from pathology gets richer and richer and richer, we might think that we can now use computers to try to better predict who is likely to benefit from a treatment than humans could do alone. But the challenge with using algorithms to do that is that people respond differently to treatment. And the data which is being used to guide treatment is biased based on existing treatment guidelines. So similarly to the previous question, we could ask what would happen if we trained to predict past treatment decisions. This would be the most naive way to try to use data to guide treatment decisions. So maybe you see David gets treatment A, John gets treatment B, Juana gets treatment A. And you might ask, then, OK, a new patient comes in. What should this new patient be treated with? And if you just learned a model to predict from what you know about David the treatment that David is likely to get, then the best that you could hope to do is to do as well as existing clinical practice. So if we want to go beyond current clinical practice, for example, to recognize that there is heterogeneity in treatment response, then we have to somehow change the question that we're asking. I'll give you one last example, which is perhaps a more traditional question of does X cause Y? For example, does smoking cause lung cancer is a major question of societal importance. Now, you might be familiar with the traditional way of trying to answer questions of this nature, which would be to do a randomized control trial. Except this isn't exactly the type of setting where you could do a randomized control trial. How would you feel if you were a smoker, and someone came up to you and said, you have to stop smoking because I need to see what happens? Or how would you feel if you were a non-smoker, and someone came up to you and said, you have to start smoking? That would be both not feasible and completely unethical. And so if we want to try to answer questions like this from data, we need to start thinking about how can we design, using observational data, ways of answering questions like this. And the challenge is that there's going to be bias in the data because of who decides to smoke and who decides not to smoke. So for example, the most naive way you might try to answer this question would be to look at the conditional likelihood getting lung cancer among smokers and getting lung cancer among non-smokers. But those numbers, as you'll see in the next few slides, can be very misleading because there might be confounding factors, factors that would, for example, both cause people to be a smoker and cause them to receive lung cancer, which would differentiate between these two numbers. And we'll have a very concrete example of this in just a few minutes. So to properly answer all of these questions, one needs to be thinking in terms of causal graphs. So rather than the traditional setup in machine learning, where you just have inputs and outputs, now we need to have triplets. Rather than having inputs and outputs, we need to be thinking of inputs, intervention."
}