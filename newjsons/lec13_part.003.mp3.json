{
    "chunks": [
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 0.0,
            "end": 11.22,
            "text": " Tire Q-sic is the kind of prior state-of-the-art model.  It's a model based out of the UK.  They're developed by someone named Sir Q-sic, who's  knighted over this work.  It's very commonly used."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 11.22,
            "end": 22.38,
            "text": " So that one had an AUC of 62.  Our image-only model had an AUC of about 68.  And the hybrid one had an AUC of 70.  So what does this kind of AUC thing  give you when you look using a risk model?"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 22.38,
            "end": 32.8,
            "text": " What it gives you is the ability to create better high risk  and low risk cohorts.  So in terms of looking at high risk cohorts,  our best model plays about 30% of all the cancers  in the population in the top 10%,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 32.8,
            "end": 46.36,
            "text": " and 3% of all the cancers in the bottom 10%,  compared to 18 and 5 to the prior state-of-the-art.  And so what this enables you to do, if you're going to say,  this 10% should actually qualify for MRI,  you can start fighting this problem of majority"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 46.36,
            "end": 58.0,
            "text": " of people that get cancer don't have MRI,  and the majority of people that get it don't need it.  It's all about, is your risk model actually  placed the right people into the right buckets?  Now, we saw that this trend of outperforming"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 58.0,
            "end": 69.08,
            "text": " the prior state-of-the-art held across races.  And one of the things that was kind of astonishing  was that, though Thai CREASER performed well in white women,  which makes sense because it was developed only using  white women in the UK, it was worse than random raw data"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 69.08,
            "end": 79.2,
            "text": " set for African-American women.  And so this kind of emphasizes the importance  of this kind of analysis to make sure that the kind of data  set that you have is reflective of the population you're  trying to serve, and actually doing"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 79.2,
            "end": 94.56,
            "text": " the analysis accordingly.  So we saw that our model kind of held across races,  and as well across, you know, we see this trend  across pre-postmenopause and women without family history.  One thing we did in terms of a more granular comparison"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 94.56,
            "end": 109.6,
            "text": " of performance, we looked at, if we just look at kind of like  the risk thirds for our model and the Thai CREASER model,  what's the trend that you see, or the cases where,  kind of like which one is right is kind of ambiguous.  And what I should show in these boxes is the cancer incidents,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 109.6,
            "end": 120.12,
            "text": " the prevalence in that population.  So the darker the box, the higher the incidence.  And on the right-hand side, there's random images  from cases within those boxes.  Does that make sense for everyone?"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 120.12,
            "end": 133.04,
            "text": " Great.  So a clear trend that you see is that, for example,  if TCV8 calls you high risk, but we call it low,  that is a lower incidence than if we call it medium,  and they call it low."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 133.04,
            "end": 143.6,
            "text": " So kind of like you kind of see this straight column-wise  pattern showing the discrimination truly  does follow the deep learning model and not  the classical approach.  And by looking at the random images that were selected,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 143.6,
            "end": 154.56,
            "text": " in case we disagree, it supports the notion  that it's not just that the column is just  the most dense, crazy dense-looking breast,  and that there's something more subtle that's picking up.  That's actually indicative of breast cancer risk."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 154.56,
            "end": 168.24,
            "text": " Kind of a very similar analysis we looked at  is if we look at just by traditional breast density  as labeled by the original red dotters on the development set  or on the test set, we end up seeing the same trend,  where if someone is non-dense, we call them high risk."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 168.24,
            "end": 178.64,
            "text": " They're much higher risk than someone that is dense  that we call low risk.  And as before, the kind of real next step  here to make this truly valuable and truly useful  is actually implementing clinically,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 178.64,
            "end": 187.56,
            "text": " seeing this prospectively, and with more centers  and kind of more population to see, does this work?  And does this deliver the kind of benefits  that we care about?  And viewing, what is the leverage"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 187.56,
            "end": 199.0,
            "text": " to change once you know someone is high risk?  Perhaps MRI, perhaps more frequent screening.  And so this is the kind of gap between having  a useful technology on the paper side  to an actual useful technology in real life."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 199.04,
            "end": 210.84,
            "text": " So I am moving on schedule.  So now I'm going to talk about how to mess up.  And it's actually quite interesting.  There's like so many ways.  And I fall into them a few times myself, and it happens."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 210.84,
            "end": 218.72,
            "text": " And kind of following the sketch,  you can mess up in data set collection.  It's probably the most common by far.  You can mess up in modeling, which I'm doing right now,  and it's very sad."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 218.72,
            "end": 229.64000000000001,
            "text": " And you can mess up in analysis, which is really preventable.  So in data set collection, enriched data sets  are kind of the most common thing  you see in this paper.  Or kind of the most common thing you see in the space."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 229.64000000000001,
            "end": 244.08,
            "text": " If you find a public data set, there's  more likely to be like 50-50 cancer, not cancer.  And oftentimes, these data sets collect  can have some sort of bias within the way it was collected.  So it might be that you have negative cases from less"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 244.08,
            "end": 252.04000000000002,
            "text": " centers and you have positive cases,  or they're collected from different years.  And actually, this is something we  ran into earlier in our own work.  Once upon a time, Connie and I were"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 252.04000000000002,
            "end": 265.12,
            "text": " in Shanghai for the opening of a cancer center there.  And at that time, we had all the cancers  from the MGH data set, about 2,000.  But the mammograms were still being collected annually  from 2009."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 265.12,
            "end": 277.0,
            "text": " So at that time, we only had half of the negatives by year,  but all of the cancers.  And all of a sudden, I had to like,  I came up with a slightly more complicated model,  as one often does, that looks at several images at the same time."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 277.0,
            "end": 286.36,
            "text": " And my AAC went up to like 95, and I had all this bouncing  off the wall.  And then I had some suspicion of like, wait a second.  This is too high.  This is too good."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 286.36,
            "end": 298.84000000000003,
            "text": " And we quickly realized that all these numbers  were kind of a myth.  But this level of, if you do these kind of case control  things, you can oftentimes, unless you're  very careful about the way it was constructed,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 298.84000000000003,
            "end": 310.20000000000005,
            "text": " you can easily run into these issues,  and your testing set won't protect you from that.  And so having a clean data set that truly  follows the spectrum you expect to use it in,  i.e. a natural distribution collected"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 310.20000000000005,
            "end": 321.64,
            "text": " through routine clinical care, is important to say,  will it behave as I actually want it to be used?  In general, some of this you can think through  in first principle.  But it kind of stresses the importance"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 321.64,
            "end": 330.47999999999996,
            "text": " of actually testing this prospectively  in external validation to try to see,  does this work when I take away some  of the biases in my data set?  And being really careful about that."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 330.47999999999996,
            "end": 342.32,
            "text": " The common approach of just controlling by age or by density  is not enough when the model can catch really fine-grained  signals.  How to mess up in modeling.  So there's been adventures in this space as well."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 342.32,
            "end": 354.71999999999997,
            "text": " One of the things I've recently discovered  is that the actual mammography machine device  that the machine was captured on.  So you saw a bunch of mammograms from different machines.  Has an unexpected impact on the model."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 354.71999999999997,
            "end": 365.44,
            "text": " So the actual probability distribution,  the distribution of cancer probabilities by the model  is not independent of the device.  And so something we're going through now,  we actually ran into this while working on clinical computation,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 365.44,
            "end": 379.15999999999997,
            "text": " is this kind of conditional adversarial training setup  to try to rectify this issue.  So this is much harder to catch based on first principle.  But it's important to think through as you really  start demoing out your clinical computations."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 379.15999999999997,
            "end": 389.92,
            "text": " This is where these issues pop up easily  and they're harder to avoid.  And lastly, and I think probably one  that's probably the most important,  is messing up in analysis."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 389.92,
            "end": 404.03999999999996,
            "text": " So it's quite common in the previous section of this field.  Yes?  With the adversarial training, just to understand what you do,  do you let a discriminator predict a machine?  And then you train against that?"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 404.03999999999996,
            "end": 414.71999999999997,
            "text": " So my answer is going to be two parts.  One, it doesn't work as well as I want it to yet.  So really, who knows?  But my best hunch in terms of what's  been done before for other kind of work specifically"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 414.71999999999997,
            "end": 424.2,
            "text": " in radio signals is they use a conditional adversarial.  So you feed the discriminator both a label  and image representation.  You have it try to predict out the device  to try to take away the information that's not just"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 424.2,
            "end": 435.88000000000005,
            "text": " contained within the label distribution.  And that's been shown to be very helpful for people trying  to do sleep state detection based off on Wi-Fi.  Well, not Wi-Fi, but radio waves.  In the exotobis group, but also it"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 435.88000000000005,
            "end": 445.72,
            "text": " seems to be the most common approach  I've seen in literature.  So it's something that I'm going to try soon.  I haven't implemented it.  It's just GPU time and waiting to queue up the experiment."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 445.72,
            "end": 455.52000000000004,
            "text": " And the last part in terms of how to mess up  is this kind of analysis.  One thing that's common is people  assume that synthetic experiments are the same thing  as chemical mutation."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 455.52,
            "end": 466.4,
            "text": " People do reader studies very often.  And it's quite common to see that when you do reader studies,  you might find that computer detection does a huge difference  in reader studies.  And it's a connection to show it was harmful in real life."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 466.4,
            "end": 480.12,
            "text": " And it's important to do these real world experiments that  really say what is happening and just give them the real benefit  that I expected.  And a hopefully less common nowadays mistake  is that oftentimes people exclude"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 480.12,
            "end": 488.88,
            "text": " all inconvenient cases.  So there was a paper yesterday that just came out  that did cancer detection using kind  of this patch level architecture.  But if you read more closely into their details,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 488.88,
            "end": 499.68,
            "text": " they excluded all women with breasts  that they considered too small by some threshold  for modern convenience.  But that might disproportionately affect  specifically Asian women in that population."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 499.68,
            "end": 510.84000000000003,
            "text": " And so they didn't do a subgroup analysis  for all the different races.  So it's hard to know what is happening there.  If your population is mostly white, which it is at MGH  and is a lot of the centers that these colleges have developed,"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 510.84000000000003,
            "end": 518.68,
            "text": " reporting the average that you see  isn't enough to really validate that.  And so you can have things like a tycusic model  that are worse than random and explicitly harmful  for African-American women."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 518.68,
            "end": 530.04,
            "text": " And so guarding against that is you  can do a lot of that based on first principle.  But some of these things you can only really find out  by actively monitoring to say, is there  any subpopulation that I didn't think about a priori that"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 530.04,
            "end": 541.48,
            "text": " could be harmed?  And finally, so talk a lot about clinical deployments.  We've actually done this a couple times.  And we're going to switch over to Connie real soon.  In general, what you want to do is"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 541.48,
            "end": 554.48,
            "text": " you want to make it as easy, as plausible,  and possible for the in-house IT team to use your tool.  We've gone through this with now like,  I don't know, it depends how you count.  Like once for density and then like three times"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 554.48,
            "end": 566.88,
            "text": " at the same time.  And I spent like many hours sitting there.  And the broad way that we've set it up so far  is we just have a kind of dockerized container  to manage a web app that holds the model."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 566.88,
            "end": 576.66,
            "text": " This web app has kind of a docker processing toolkit.  So the kind of steps that all of our deployments follow  and are like under a unified framework  is the IT application will get some images out of the PAC  system."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 576.66,
            "end": 586.26,
            "text": " It will send it over to application.  We're going to convert it to the PNG in the way  that we expect because we kind of encapsulate  this functionality, run through the model, send it back,  and then write it back to the HR."
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 586.26,
            "end": 599.54,
            "text": " One of the things I ran into was that they didn't actually  know how to use things like HTTP because it's not actually  normal within their infrastructure.  And so being cognizant that some of these more like tech  standard things of like just HTTP requests and like"
        },
        {
            "number": "lec13",
            "title": "part.003.mp3",
            "start": 599.54,
            "end": 601.62,
            "text": " responses and stuff."
        }
    ],
    "text": " Tire Q-sic is the kind of prior state-of-the-art model. It's a model based out of the UK. They're developed by someone named Sir Q-sic, who's knighted over this work. It's very commonly used. So that one had an AUC of 62. Our image-only model had an AUC of about 68. And the hybrid one had an AUC of 70. So what does this kind of AUC thing give you when you look using a risk model? What it gives you is the ability to create better high risk and low risk cohorts. So in terms of looking at high risk cohorts, our best model plays about 30% of all the cancers in the population in the top 10%, and 3% of all the cancers in the bottom 10%, compared to 18 and 5 to the prior state-of-the-art. And so what this enables you to do, if you're going to say, this 10% should actually qualify for MRI, you can start fighting this problem of majority of people that get cancer don't have MRI, and the majority of people that get it don't need it. It's all about, is your risk model actually placed the right people into the right buckets? Now, we saw that this trend of outperforming the prior state-of-the-art held across races. And one of the things that was kind of astonishing was that, though Thai CREASER performed well in white women, which makes sense because it was developed only using white women in the UK, it was worse than random raw data set for African-American women. And so this kind of emphasizes the importance of this kind of analysis to make sure that the kind of data set that you have is reflective of the population you're trying to serve, and actually doing the analysis accordingly. So we saw that our model kind of held across races, and as well across, you know, we see this trend across pre-postmenopause and women without family history. One thing we did in terms of a more granular comparison of performance, we looked at, if we just look at kind of like the risk thirds for our model and the Thai CREASER model, what's the trend that you see, or the cases where, kind of like which one is right is kind of ambiguous. And what I should show in these boxes is the cancer incidents, the prevalence in that population. So the darker the box, the higher the incidence. And on the right-hand side, there's random images from cases within those boxes. Does that make sense for everyone? Great. So a clear trend that you see is that, for example, if TCV8 calls you high risk, but we call it low, that is a lower incidence than if we call it medium, and they call it low. So kind of like you kind of see this straight column-wise pattern showing the discrimination truly does follow the deep learning model and not the classical approach. And by looking at the random images that were selected, in case we disagree, it supports the notion that it's not just that the column is just the most dense, crazy dense-looking breast, and that there's something more subtle that's picking up. That's actually indicative of breast cancer risk. Kind of a very similar analysis we looked at is if we look at just by traditional breast density as labeled by the original red dotters on the development set or on the test set, we end up seeing the same trend, where if someone is non-dense, we call them high risk. They're much higher risk than someone that is dense that we call low risk. And as before, the kind of real next step here to make this truly valuable and truly useful is actually implementing clinically, seeing this prospectively, and with more centers and kind of more population to see, does this work? And does this deliver the kind of benefits that we care about? And viewing, what is the leverage to change once you know someone is high risk? Perhaps MRI, perhaps more frequent screening. And so this is the kind of gap between having a useful technology on the paper side to an actual useful technology in real life. So I am moving on schedule. So now I'm going to talk about how to mess up. And it's actually quite interesting. There's like so many ways. And I fall into them a few times myself, and it happens. And kind of following the sketch, you can mess up in data set collection. It's probably the most common by far. You can mess up in modeling, which I'm doing right now, and it's very sad. And you can mess up in analysis, which is really preventable. So in data set collection, enriched data sets are kind of the most common thing you see in this paper. Or kind of the most common thing you see in the space. If you find a public data set, there's more likely to be like 50-50 cancer, not cancer. And oftentimes, these data sets collect can have some sort of bias within the way it was collected. So it might be that you have negative cases from less centers and you have positive cases, or they're collected from different years. And actually, this is something we ran into earlier in our own work. Once upon a time, Connie and I were in Shanghai for the opening of a cancer center there. And at that time, we had all the cancers from the MGH data set, about 2,000. But the mammograms were still being collected annually from 2009. So at that time, we only had half of the negatives by year, but all of the cancers. And all of a sudden, I had to like, I came up with a slightly more complicated model, as one often does, that looks at several images at the same time. And my AAC went up to like 95, and I had all this bouncing off the wall. And then I had some suspicion of like, wait a second. This is too high. This is too good. And we quickly realized that all these numbers were kind of a myth. But this level of, if you do these kind of case control things, you can oftentimes, unless you're very careful about the way it was constructed, you can easily run into these issues, and your testing set won't protect you from that. And so having a clean data set that truly follows the spectrum you expect to use it in, i.e. a natural distribution collected through routine clinical care, is important to say, will it behave as I actually want it to be used? In general, some of this you can think through in first principle. But it kind of stresses the importance of actually testing this prospectively in external validation to try to see, does this work when I take away some of the biases in my data set? And being really careful about that. The common approach of just controlling by age or by density is not enough when the model can catch really fine-grained signals. How to mess up in modeling. So there's been adventures in this space as well. One of the things I've recently discovered is that the actual mammography machine device that the machine was captured on. So you saw a bunch of mammograms from different machines. Has an unexpected impact on the model. So the actual probability distribution, the distribution of cancer probabilities by the model is not independent of the device. And so something we're going through now, we actually ran into this while working on clinical computation, is this kind of conditional adversarial training setup to try to rectify this issue. So this is much harder to catch based on first principle. But it's important to think through as you really start demoing out your clinical computations. This is where these issues pop up easily and they're harder to avoid. And lastly, and I think probably one that's probably the most important, is messing up in analysis. So it's quite common in the previous section of this field. Yes? With the adversarial training, just to understand what you do, do you let a discriminator predict a machine? And then you train against that? So my answer is going to be two parts. One, it doesn't work as well as I want it to yet. So really, who knows? But my best hunch in terms of what's been done before for other kind of work specifically in radio signals is they use a conditional adversarial. So you feed the discriminator both a label and image representation. You have it try to predict out the device to try to take away the information that's not just contained within the label distribution. And that's been shown to be very helpful for people trying to do sleep state detection based off on Wi-Fi. Well, not Wi-Fi, but radio waves. In the exotobis group, but also it seems to be the most common approach I've seen in literature. So it's something that I'm going to try soon. I haven't implemented it. It's just GPU time and waiting to queue up the experiment. And the last part in terms of how to mess up is this kind of analysis. One thing that's common is people assume that synthetic experiments are the same thing as chemical mutation. People do reader studies very often. And it's quite common to see that when you do reader studies, you might find that computer detection does a huge difference in reader studies. And it's a connection to show it was harmful in real life. And it's important to do these real world experiments that really say what is happening and just give them the real benefit that I expected. And a hopefully less common nowadays mistake is that oftentimes people exclude all inconvenient cases. So there was a paper yesterday that just came out that did cancer detection using kind of this patch level architecture. But if you read more closely into their details, they excluded all women with breasts that they considered too small by some threshold for modern convenience. But that might disproportionately affect specifically Asian women in that population. And so they didn't do a subgroup analysis for all the different races. So it's hard to know what is happening there. If your population is mostly white, which it is at MGH and is a lot of the centers that these colleges have developed, reporting the average that you see isn't enough to really validate that. And so you can have things like a tycusic model that are worse than random and explicitly harmful for African-American women. And so guarding against that is you can do a lot of that based on first principle. But some of these things you can only really find out by actively monitoring to say, is there any subpopulation that I didn't think about a priori that could be harmed? And finally, so talk a lot about clinical deployments. We've actually done this a couple times. And we're going to switch over to Connie real soon. In general, what you want to do is you want to make it as easy, as plausible, and possible for the in-house IT team to use your tool. We've gone through this with now like, I don't know, it depends how you count. Like once for density and then like three times at the same time. And I spent like many hours sitting there. And the broad way that we've set it up so far is we just have a kind of dockerized container to manage a web app that holds the model. This web app has kind of a docker processing toolkit. So the kind of steps that all of our deployments follow and are like under a unified framework is the IT application will get some images out of the PAC system. It will send it over to application. We're going to convert it to the PNG in the way that we expect because we kind of encapsulate this functionality, run through the model, send it back, and then write it back to the HR. One of the things I ran into was that they didn't actually know how to use things like HTTP because it's not actually normal within their infrastructure. And so being cognizant that some of these more like tech standard things of like just HTTP requests and like responses and stuff."
}